---
title: "p8105_hw5_tp2806"
author: "Tejashree Prakash"
output: github_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(purrr)
library(ggplot2)
library(p8105.datasets)
library(broom)
library(modelr)
```


# Problem 1

#### Cleaning the data. 
```{r}
#Load data
homicide_df <- read_csv("data/homicide-data.csv")

#Add city_state variable 
homicide_df <- homicide_df %>%
  mutate(city_state = str_c(city, ", ", state))

#Add homicide variable 
homicide_df <- homicide_df %>%
  mutate(
    resolved = as.numeric(disposition == "Closed by arrest"),
    victim_age = as.numeric(victim_age),
    victim_race = fct_relevel(victim_race, "White")) %>%
  filter(!city_state %in% c("Dallas, TX", "Phoenix, AZ", "Kansas City, MO", "Tulsa, AL")) %>%
  filter(victim_race %in% c("White", "Black"))

head(homicide_df, 5)
```

#### Performing regression model for Baltimore. 
```{r}
baltimore_df <- homicide_df %>%
  filter(city_state == "Baltimore, MD")

baltimore_log <- 
  baltimore_df %>% 
  glm(resolved ~ victim_age + victim_race + victim_sex, 
      data = ., 
      family = binomial()) 

baltimore_log_results <- baltimore_log %>%
  broom::tidy() %>% 
  mutate(OR = exp(estimate)) %>%
  select(term, log_OR = estimate, OR, p.value) %>%
  filter(term == "victim_sexMale")

baltimore_log_results <- baltimore_log %>%
  tidy() %>%
  mutate(
    OR = exp(estimate),
    ci_lower = exp(estimate - 1.96 * std.error),
    ci_upper = exp(estimate + 1.96 * std.error)
  ) |>
  filter(term == "victim_sexMale") %>%
  select(term, log_OR = estimate, OR, ci_lower, ci_upper, p.value
  )

baltimore_log_results |> 
  knitr::kable(digits = 3)
```

#### Performing log for each city. 

```{r}
nest_city_results <-
  homicide_df |> 
  nest(data = -city_state) %>%
  mutate(
    models = map(data, \(df) glm(resolved ~ victim_sex + victim_age + victim_race, 
      data = df, 
      family = binomial())
      ),
    results = map(models, broom::tidy)
    ) %>%
  select(-data, -models) %>%
  unnest(results) %>%
  mutate(OR = exp(estimate)) %>% 
  filter(term == "victim_sexMale") %>% 
  group_by(city_state) %>%
  mutate(
    ci_lower = exp(estimate - 1.96 * std.error),
    ci_upper = exp(estimate + 1.96 * std.error)
  ) %>%
  ungroup()

nest_city_results %>% 
  arrange(OR) %>%
  select(city_state, OR, p.value, ci_lower, ci_upper) %>%
  knitr::kable(digits = 3)

#Order the dataframe by descending OR 
city_or_df <- 
  nest_city_results %>% 
  filter(term == "victim_sexMale") %>% 
  arrange(OR) %>%
  mutate(city_state = factor(city_state, levels = city_state))

#Produce Plot 
ggplot(city_or_df, aes(x = city_state, y = OR)) +
  geom_point(size = 1.5, color = "navyblue") +
  geom_errorbar(aes(ymin = ci_lower, ymax = ci_upper), width = 0.2) +
  geom_hline(yintercept = 1, linetype = "dashed", color = "red") +
  coord_flip() +
  labs(
    title = "ORs for Homicide Resolution between Male vs Female Victims",
    x = "City",
    y = "Odds Ratio (Male vs Female Victims)"
  ) +
  theme_minimal()
```

The plot above demonstrates the odds ratios for homicide resolution for cases with male victims to cases with female victims. Albuquerque, NM contained the largest OR, indicating that, amongst all of the cities in this dataset, it has the greatest odds of solving homicides with male victims compared to female victims. New York, NY had the smallest OR, indicating it has the lowest odds of solving homicides with male victims compared to female victims. The width of the confidence interval bars indicates the precision of each estimate. Therefore, Fresno, Stockton, and Albuquerque has relatively lower precision due to the widest CIs. 


# Problem 2 

#### Boostrapping for Estimates

```{r}
data("weather_df")

#Bootstrap function with the estimates 
bootstrap_fn <- function(df) {
  boot_df <- df %>% 
    sample_frac(size = 1, replace = TRUE)
  fit <- lm(tmax ~ tmin + prcp, data = boot_df) #fit model
  r2 <- glance(fit)$r.squared 
  coefs <- tidy(fit) 
  beta1 <- coefs$estimate[coefs$term == "tmin"] #the two predictors for betas
  beta2 <- coefs$estimate[coefs$term == "prcp"]
  tibble( #make table
    r2 = r2,
    beta1_beta2 = beta1 / beta2
  )
}

#Run 5000 bootstrap samples with the r2 and betas estimates 
set.seed(1)
bootstrap_results <-
  map_dfr(1:5000, ~ bootstrap_fn(weather_df))

bootstrap_results %>%
  head(10) %>%
  knitr::kable(digits = 3)

bootstrap_results %>%
  pivot_longer(everything(),
               names_to = "parameter",
               values_to = "estimate") %>%
  ggplot(aes(x = estimate)) +
  geom_histogram(bins = 50, color = "white") +
  facet_wrap(~ parameter, scales = "free") +
  theme_minimal(base_size = 14) +
  labs(title = "Distributions of r² and β1/β2",
       x = "Bootstrapped Estimates",
       y = "Count")
```
The distributions for r2 and the Betas differ; the Betas distribution is left skewed, while the r2 distribution is normal.

#### Obtaining CIs

```{r}
bootstrap_results <- bootstrap_results %>%
  summarize(
    r2_lower = quantile(r2, 0.025),
    r2_upper = quantile(r2, 0.975),
    betas_lower = quantile(beta1_beta2, 0.025),
    betas_upper = quantile(beta1_beta2, 0.975)
  )

bootstrap_results %>%
  knitr::kable(digits = 3)
```

#Problem 3

#### Data Cleaning

```{r}
#Load data
birthweight_df <- read_csv("data/birthweight.csv")

#Convert categorical variables to factors (continuous remains continuous)
birthweight_df <- 
  birthweight_df %>%
  mutate(
    babysex = 
      factor(
        babysex, 
        levels = c(1, 2), 
        labels = c("Male", "Female")),
    frace = 
      factor(
        frace, 
        levels = c(1, 2, 3, 4, 8, 9), 
        labels = c("White", "Black", "Asian", "Puerto Rican", "Other", "Unknown")),
    malform = 
      factor(
        malform, 
        levels = c(0, 1), 
        labels = c("Absent", "Present")), 
    mrace = 
      factor(
        mrace, 
        levels = c(1, 2, 3, 4, 8), 
        labels = c("White", "Black", "Asian", "Puerto Rican", "Other")
  ))
    
#Check for NAs --> none!
colSums(is.na(birthweight_df))
```

#### First Regression Model 

```{r}
main_model <- 
  birthweight_df %>% 
  lm(bwt ~ delwt + mheight + momage + mrace + parity + ppwt + pnumlbw + pnumsga + smoken + wtgain, data = .) 

#Model
main_model %>%
  broom::tidy() %>%
  knitr::kable(digits = 3)

#Removing predictors with NA estimates because it indicates multicollinearity 
main_model_full <- 
  birthweight_df %>% 
  lm(bwt ~ delwt + mheight + momage + mrace + parity + ppwt + smoken, data = .) 
main_model_full %>%
  broom::tidy() %>%
  knitr::kable(digits = 3, caption = "Main Model")

#Add residuals
birthweight_df <- birthweight_df %>%
  add_predictions(main_model_full) %>%
  add_residuals(main_model_full)

#Plot residuals
ggplot(birthweight_df, aes(x = pred, y = resid)) +
  geom_point(alpha = 0.5, shape = 21, color = "lightgrey", fill = "skyblue") +
  geom_hline(yintercept = 0, linetype = "dashed", color = "red") +
  theme_minimal(base_size = 14) +
  labs(
    title = "Residuals vs Fitted Values",
    x = "Fitted Values",
    y = "Residuals"
  )
```
This regression model is built with a hypothesized structure for the parental factors that underly birthweight based off of academic research. According to research, the age, race, and prenatal care that the mother experienced are strong factors that influence birthweight. Mother's attributes are thought to be more influential than father's attributes Additionally, previous pregnancy history is thought to affect birth outcomes as well. I also used a social determinants of health framework. References: https://web.stanford.edu/group/virus/herpes/2000/primaryf.htm <br>

After running the initial model, I removed the parameters that demonstrated multicollinearity and ran the model again. From this main model, the residuals are mostly centered around zero with a visually random scatter, suggesting homoscedasicity. This indicates this model is reasonable. 



#### Constructing Other Models

```{r}
#Make length and age model 
length_age_model <- 
  birthweight_df %>% 
  lm(bwt ~ blength + gaweeks, data = .) 

length_age_model %>%
  broom::tidy() %>%
  knitr::kable(digits = 3, caption = "Length and Gestational Age Model")

#Make head circumference, length, sex, and all interactions model
interactions_model <- 
  birthweight_df %>% 
  lm(bwt ~ bhead * blength * babysex + bhead:blength + bhead:babysex + blength:babysex +
     bhead:blength:babysex, data = .) 

interactions_model %>%
  broom::tidy() %>%
  knitr::kable(digits = 3, caption = "Head + Length + Sex with Interactions Model")
```


#### Comparing Models 

```{r}
set.seed(1)

cv_df =
  crossv_mc(birthweight_df, n = 100) |>
  mutate(
    train = map(train, as_tibble),
    test  = map(test, as_tibble),

    fit_main   = map(train, ~ lm(bwt ~ delwt + mheight + momage + mrace + parity + ppwt + smoken, data = .x)),
    fit_ga     = map(train, ~ lm(bwt ~ blength+gaweeks, data = .x)),
    fit_inter  = map(train, ~ lm(bwt ~ bhead * blength * babysex + bhead:blength + bhead:babysex +       blength:babysex + bhead:blength:babysex, data = .x)),

    rmse_main  = map2_dbl(fit_main,  test, ~ sqrt(mean((predict(.x, .y) - .y$bwt)^2))),
    rmse_length_age    = map2_dbl(fit_ga,    test, ~ sqrt(mean((predict(.x, .y) - .y$bwt)^2))),
    rmse_interactions = map2_dbl(fit_inter, test, ~ sqrt(mean((predict(.x, .y) - .y$bwt)^2)))
  )

#Table
cv_df %>%
  summarize(
    mean_rmse_main  = mean(rmse_main),
    mean_rmse_ga    = mean(rmse_length_age),
    mean_rmse_inter = mean(rmse_interactions)
  ) %>%
  knitr::kable(digits = 3, caption = "Cross-Validated Model Comparison")

#Plot
cv_df %>%
  select(rmse_main, rmse_length_age, rmse_interactions) %>%
  pivot_longer(everything(), names_to = "model", values_to = "rmse") %>%
  ggplot(aes(x = model, y = rmse)) +
  geom_boxplot(fill = "lightblue", alpha = 0.5) +
  labs(
    title = "Cross-Validation for All Models",
    x = "Model",
    y = "RMSE"
  )
```
The Cross-Validated RMSE shows that the main model has the highest prediction error compared to the length and age model and the interactions model. The interactions model had the lowest RMSE, indicating that it has the better predictive ability relative to the other two models. 

